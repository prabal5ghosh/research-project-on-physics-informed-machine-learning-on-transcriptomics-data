{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bade9f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 73\u001b[0m\n\u001b[0;32m     70\u001b[0m theta \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m2\u001b[39m, num_genes, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Example: 2 parameters per gene\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m train(model, t_tensor, X_tensor, theta)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Predictions\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "Cell \u001b[1;32mIn[2], line 59\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, t, X, theta, epochs, lr)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     58\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 59\u001b[0m     loss \u001b[38;5;241m=\u001b[39m compute_loss(model, t, X, theta)\n\u001b[0;32m     60\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     61\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[1;32mIn[2], line 48\u001b[0m, in \u001b[0;36mcompute_loss\u001b[1;34m(model, t, X, theta)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_loss\u001b[39m(model, t, X, theta):\n\u001b[0;32m     47\u001b[0m     x_pred \u001b[38;5;241m=\u001b[39m model(X)\n\u001b[1;32m---> 48\u001b[0m     dxdt_pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(x_pred, t, grad_outputs\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mones_like(x_pred), create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     49\u001b[0m     dxdt_ode \u001b[38;5;241m=\u001b[39m ode_model(t, x_pred, theta)\n\u001b[0;32m     50\u001b[0m     data_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean((x_pred \u001b[38;5;241m-\u001b[39m X) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:411\u001b[0m, in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[0;32m    407\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[0;32m    408\u001b[0m         grad_outputs_\n\u001b[0;32m    409\u001b[0m     )\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 411\u001b[0m     result \u001b[38;5;241m=\u001b[39m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    412\u001b[0m         t_outputs,\n\u001b[0;32m    413\u001b[0m         grad_outputs_,\n\u001b[0;32m    414\u001b[0m         retain_graph,\n\u001b[0;32m    415\u001b[0m         create_graph,\n\u001b[0;32m    416\u001b[0m         inputs,\n\u001b[0;32m    417\u001b[0m         allow_unused,\n\u001b[0;32m    418\u001b[0m         accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    419\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[0;32m    421\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m    422\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[0;32m    423\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[0;32m    424\u001b[0m     ):\n",
      "\u001b[1;31mRuntimeError\u001b[0m: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data\n",
    "data = np.array([\n",
    "    [-0.44796003, -0.560154646, -5.091964284],\n",
    "    [1.778358524, 0, 0],\n",
    "    [0, 1.175149691, 0],\n",
    "    [0.055374646, 0.098434984, -0.101756864],\n",
    "    [0.505, 0.505, 0.505]\n",
    "])\n",
    "\n",
    "time_points = np.array([1, 2, 4])  # Time points in days\n",
    "num_genes = data.shape[0]\n",
    "\n",
    "# Prepare data for LSTM (reshape to [sequence length, batch size, input size])\n",
    "t = time_points.reshape(-1, 1).astype(np.float32)  # Reshape time points to [3, 1]\n",
    "X = data.T.reshape(-1, 3, num_genes).astype(np.float32)  # Reshape data to [3, 5] -> [sequence length, batch size, input size]\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "t_tensor = torch.tensor(t, requires_grad=True)\n",
    "X_tensor = torch.tensor(X)\n",
    "\n",
    "# Neural Network Model with LSTM and Dropout\n",
    "class PINN_LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_rate=0.2):\n",
    "        super(PINN_LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)  # LSTM output\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        x = self.fc(x)  # Fully connected layer\n",
    "        return x\n",
    "\n",
    "# ODE Model\n",
    "def ode_model(t, x, theta):\n",
    "    return theta[0] * x + theta[1]  # Example: dx/dt = theta[0] * x + theta[1]\n",
    "\n",
    "# Loss Function\n",
    "def compute_loss(model, t, X, theta):\n",
    "    x_pred = model(X)\n",
    "    dxdt_pred = torch.autograd.grad(x_pred, t, grad_outputs=torch.ones_like(x_pred), create_graph=True)[0]\n",
    "    dxdt_ode = ode_model(t, x_pred, theta)\n",
    "    data_loss = torch.mean((x_pred - X) ** 2)\n",
    "    physics_loss = torch.mean((dxdt_pred - dxdt_ode) ** 2)\n",
    "    return data_loss + physics_loss\n",
    "\n",
    "# Training\n",
    "def train(model, t, X, theta, epochs=100, lr=0.01):\n",
    "    optimizer = optim.Adam(list(model.parameters()) + [theta], lr=lr)\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        loss = compute_loss(model, t, X, theta)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "# Initialize model and parameters\n",
    "input_size = num_genes\n",
    "hidden_size = 10\n",
    "output_size = num_genes\n",
    "model = PINN_LSTM(input_size, hidden_size, output_size)\n",
    "theta = torch.randn(2, num_genes, requires_grad=True)  # Example: 2 parameters per gene\n",
    "\n",
    "# Train the model\n",
    "train(model, t_tensor, X_tensor, theta)\n",
    "\n",
    "# Predictions\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_tensor).numpy()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i in range(num_genes):\n",
    "    plt.subplot(num_genes, 1, i+1)\n",
    "    plt.plot(time_points, data[i], 'bo-', label=f'Actual Gene {i+1}')\n",
    "    plt.plot(time_points, predictions[:, 0, i], 'r--', label=f'Predicted Gene {i+1}')\n",
    "    plt.xlabel('Time (days)')\n",
    "    plt.ylabel('Expression Level')\n",
    "    plt.legend()\n",
    "    plt.title(f'Gene {i+1} Expression Over Time')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Estimated parameters\n",
    "print(\"Estimated parameters:\", theta.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab1574b",
   "metadata": {},
   "source": [
    "### Derivation of the Ordinary Differential Equation (ODE) for Gene Expression Data\n",
    "\n",
    "We are given the following time-series gene expression data for `AT5G40100`:\n",
    "\n",
    "| Time (days) | Expression Level |\n",
    "|-------------|------------------|\n",
    "| 1           | -0.44796003      |\n",
    "| 2           | -0.560154646     |\n",
    "| 4           | -5.091964284     |\n",
    "\n",
    "#### Step 1: Analyze the Data\n",
    "The expression level of `AT5G40100` decreases over time, with a significant drop between day 2 and day 4. This suggests that the gene expression follows a decay process.\n",
    "\n",
    "#### Step 2: Assume a Simple ODE Model\n",
    "A common model for decay processes is the **exponential decay model**, which can be described by the following ODE:\n",
    "\n",
    "\\[\n",
    "\\frac{dx}{dt} = -kx\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( x \\) is the gene expression level.\n",
    "- \\( t \\) is time.\n",
    "- \\( k \\) is the decay rate constant.\n",
    "\n",
    "#### Step 3: Solve the ODE\n",
    "The solution to the ODE \\(\\frac{dx}{dt} = -kx\\) is:\n",
    "\n",
    "\\[\n",
    "x(t) = x_0 e^{-kt}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( x_0 \\) is the initial gene expression level at \\( t = 0 \\).\n",
    "\n",
    "#### Step 4: Fit the Data to the Model\n",
    "Using the given data points, we can estimate the parameters \\( x_0 \\) and \\( k \\).\n",
    "\n",
    "1. At \\( t = 1 \\), \\( x(1) = -0.44796003 \\):\n",
    "   \\[\n",
    "   -0.44796003 = x_0 e^{-k \\cdot 1}\n",
    "   \\]\n",
    "\n",
    "2. At \\( t = 2 \\), \\( x(2) = -0.560154646 \\):\n",
    "   \\[\n",
    "   -0.560154646 = x_0 e^{-k \\cdot 2}\n",
    "   \\]\n",
    "\n",
    "3. At \\( t = 4 \\), \\( x(4) = -5.091964284 \\):\n",
    "   \\[\n",
    "   -5.091964284 = x_0 e^{-k \\cdot 4}\n",
    "   \\]\n",
    "\n",
    "#### Step 5: Estimate Parameters\n",
    "To estimate \\( x_0 \\) and \\( k \\), we can use the first two data points:\n",
    "\n",
    "1. Divide the equation for \\( t = 2 \\) by the equation for \\( t = 1 \\):\n",
    "   \\[\n",
    "   \\frac{-0.560154646}{-0.44796003} = \\frac{x_0 e^{-2k}}{x_0 e^{-k}}\n",
    "   \\]\n",
    "   \\[\n",
    "   1.2504 = e^{-k}\n",
    "   \\]\n",
    "   Taking the natural logarithm:\n",
    "   \\[\n",
    "   \\ln(1.2504) = -k\n",
    "   \\]\n",
    "   \\[\n",
    "   k \\approx -0.223\n",
    "   \\]\n",
    "\n",
    "2. Substitute \\( k \\) back into the equation for \\( t = 1 \\) to find \\( x_0 \\):\n",
    "   \\[\n",
    "   -0.44796003 = x_0 e^{-0.223 \\cdot 1}\n",
    "   \\]\n",
    "   \\[\n",
    "   x_0 \\approx -0.44796003 / e^{-0.223}\n",
    "   \\]\n",
    "   \\[\n",
    "   x_0 \\approx -0.44796003 / 0.800\n",
    "   \\]\n",
    "   \\[\n",
    "   x_0 \\approx -0.560\n",
    "   \\]\n",
    "\n",
    "#### Step 6: Write the ODE\n",
    "Based on the analysis, the ODE for the gene expression of `AT5G40100` is:\n",
    "\n",
    "\\[\n",
    "\\frac{dx}{dt} = -0.223x\n",
    "\\]\n",
    "\n",
    "#### Step 7: Verify the Model\n",
    "Using the estimated parameters, we can verify the model at \\( t = 4 \\):\n",
    "\\[\n",
    "x(4) = -0.560 e^{-0.223 \\cdot 4}\n",
    "\\]\n",
    "\\[\n",
    "x(4) \\approx -0.560 e^{-0.892}\n",
    "\\]\n",
    "\\[\n",
    "x(4) \\approx -0.560 \\cdot 0.410\n",
    "\\]\n",
    "\\[\n",
    "x(4) \\approx -0.230\n",
    "\\]\n",
    "\n",
    "The predicted value at \\( t = 4 \\) is \\(-0.230\\), which differs from the observed value of \\(-5.091964284\\). This suggests that the simple exponential decay model may not fully capture the dynamics, and a more complex model (e.g., including additional terms) may be needed.\n",
    "\n",
    "---\n",
    "\n",
    "### Final ODE\n",
    "The derived ODE for the gene expression of `AT5G40100` is:\n",
    "\n",
    "\\[\n",
    "\\frac{dx}{dt} = -0.223x\n",
    "\\]\n",
    "\n",
    "This ODE describes the decay of gene expression over time, with a decay rate constant of \\( k = 0.223 \\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4afe9c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
